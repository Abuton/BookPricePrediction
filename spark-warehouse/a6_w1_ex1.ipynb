{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome to exercise one of “Apache Spark for Scalable Machine Learning on BigData”. In this exercise you’ll apply the basics of functional and parallel programming. \n",
    "\n",
    "Let’s start with a simple example. Let’s consider you have a list of integers.\n",
    "\n",
    "Let’s find out what the size of this list is.\n",
    "\n",
    "Note that we already provide an RDD object, so please have a look at the RDD API in order to find out what function to use:\n",
    "[https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "\n",
    "The following link contains additional documentation:\n",
    "[https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/Environment-Setup>\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/FAQ>\n",
    "\n",
    "Then, please feel free to ask:\n",
    "\n",
    "[https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all](https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0201EN-SkillsNetwork-20647446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "<https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me>\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==2.4.5\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\http\\client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\http\\client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 224, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 180, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 321, in run\n",
      "    reqs, check_supported_wheels=not options.target_dir\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 122, in resolve\n",
      "    requirements, max_rounds=try_to_avoid_resolution_too_deep,\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 445, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 339, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name, criterion)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 207, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_criteria_to_update(candidate)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 198, in _get_criteria_to_update\n",
      "    for r in self._p.get_dependencies(candidate):\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\provider.py\", line 172, in get_dependencies\n",
      "    for r in candidate.iter_dependencies(with_requires)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\provider.py\", line 171, in <listcomp>\n",
      "    r\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 257, in iter_dependencies\n",
      "    requires = self.dist.requires() if with_requires else ()\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 239, in dist\n",
      "    self._prepare()\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 319, in _prepare_distribution\n",
      "    self._ireq, parallel_builds=True,\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 480, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 505, in _prepare_linked_requirement\n",
      "    self.download_dir, hashes,\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 257, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 130, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 168, in iter\n",
      "    for x in it:\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 88, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"c:\\users\\abuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x+1).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 101)).reduce(lambda a,b: b+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please replace $$ with the correct characters\n",
    "rdd.c$$$t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see \"100\" as answer. Now we want to know the sum of all elements. Please again, have a look at the API documentation and complete the code below in order to get the sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.s$$()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get \"4950\" as answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by <a href=\"https://linkedin.com/in/romeo-kienzler-089b4557\"> Romeo Kienzler </a>  I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-29        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
