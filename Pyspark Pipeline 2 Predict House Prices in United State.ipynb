{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MLPipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- statezip: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data.csv', inferSchema=True, header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+----------------+--------+-------+\n",
      "|               date|    price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|sqft_above|sqft_basement|yr_built|yr_renovated|              street|            city|statezip|country|\n",
      "+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+----------------+--------+-------+\n",
      "|2014-05-02 00:00:00| 313000.0|     3.0|      1.5|       1340|    7912|   1.5|         0|   0|        3|      1340|            0|    1955|        2005|18810 Densmore Ave N|       Shoreline|WA 98133|    USA|\n",
      "|2014-05-02 00:00:00|2384000.0|     5.0|      2.5|       3650|    9050|   2.0|         0|   4|        5|      3370|          280|    1921|           0|     709 W Blaine St|         Seattle|WA 98119|    USA|\n",
      "|2014-05-02 00:00:00| 342000.0|     3.0|      2.0|       1930|   11947|   1.0|         0|   0|        4|      1930|            0|    1966|           0|26206-26214 143rd...|            Kent|WA 98042|    USA|\n",
      "|2014-05-02 00:00:00| 420000.0|     3.0|     2.25|       2000|    8030|   1.0|         0|   0|        4|      1000|         1000|    1963|           0|     857 170th Pl NE|        Bellevue|WA 98008|    USA|\n",
      "|2014-05-02 00:00:00| 550000.0|     4.0|      2.5|       1940|   10500|   1.0|         0|   0|        4|      1140|          800|    1976|        1992|   9105 170th Ave NE|         Redmond|WA 98052|    USA|\n",
      "|2014-05-02 00:00:00| 490000.0|     2.0|      1.0|        880|    6380|   1.0|         0|   0|        3|       880|            0|    1938|        1994|      522 NE 88th St|         Seattle|WA 98115|    USA|\n",
      "|2014-05-02 00:00:00| 335000.0|     2.0|      2.0|       1350|    2560|   1.0|         0|   0|        3|      1350|            0|    1976|           0|   2616 174th Ave NE|         Redmond|WA 98052|    USA|\n",
      "|2014-05-02 00:00:00| 482000.0|     4.0|      2.5|       2710|   35868|   2.0|         0|   0|        3|      2710|            0|    1989|           0|   23762 SE 253rd Pl|    Maple Valley|WA 98038|    USA|\n",
      "|2014-05-02 00:00:00| 452500.0|     3.0|      2.5|       2430|   88426|   1.0|         0|   0|        4|      1570|          860|    1985|           0|46611-46625 SE 12...|      North Bend|WA 98045|    USA|\n",
      "|2014-05-02 00:00:00| 640000.0|     4.0|      2.0|       1520|    6200|   1.5|         0|   0|        3|      1520|            0|    1945|        2010|    6811 55th Ave NE|         Seattle|WA 98115|    USA|\n",
      "|2014-05-02 00:00:00| 463000.0|     3.0|     1.75|       1710|    7320|   1.0|         0|   0|        3|      1710|            0|    1948|        1994|  Burke-Gilman Trail|Lake Forest Park|WA 98155|    USA|\n",
      "|2014-05-02 00:00:00|1400000.0|     4.0|      2.5|       2920|    4000|   1.5|         0|   0|        5|      1910|         1010|    1909|        1988|3838-4098 44th Av...|         Seattle|WA 98105|    USA|\n",
      "|2014-05-02 00:00:00| 588500.0|     3.0|     1.75|       2330|   14892|   1.0|         0|   0|        3|      1970|          360|    1980|           0|    1833 220th Pl NE|       Sammamish|WA 98074|    USA|\n",
      "|2014-05-02 00:00:00| 365000.0|     3.0|      1.0|       1090|    6435|   1.0|         0|   0|        4|      1090|            0|    1955|        2009| 2504 SW Portland Ct|         Seattle|WA 98106|    USA|\n",
      "|2014-05-02 00:00:00|1200000.0|     5.0|     2.75|       2910|    9480|   1.5|         0|   0|        3|      2910|            0|    1939|        1969|    3534 46th Ave NE|         Seattle|WA 98105|    USA|\n",
      "|2014-05-02 00:00:00| 242500.0|     3.0|      1.5|       1200|    9720|   1.0|         0|   0|        4|      1200|            0|    1965|           0|   14034 SE 201st St|            Kent|WA 98042|    USA|\n",
      "|2014-05-02 00:00:00| 419000.0|     3.0|      1.5|       1570|    6700|   1.0|         0|   0|        4|      1570|            0|    1956|           0|     15424 SE 9th St|        Bellevue|WA 98007|    USA|\n",
      "|2014-05-02 00:00:00| 367500.0|     4.0|      3.0|       3110|    7231|   2.0|         0|   0|        3|      3110|            0|    1997|           0|   11224 SE 306th Pl|          Auburn|WA 98092|    USA|\n",
      "|2014-05-02 00:00:00| 257950.0|     3.0|     1.75|       1370|    5858|   1.0|         0|   0|        3|      1370|            0|    1987|        2000|     1605 S 245th Pl|      Des Moines|WA 98198|    USA|\n",
      "|2014-05-02 00:00:00| 275000.0|     3.0|      1.5|       1180|   10277|   1.0|         0|   0|        3|      1180|            0|    1983|        2009|  12425 415th Ave SE|      North Bend|WA 98045|    USA|\n",
      "+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------+--------+-------+\n",
      "|summary|            price|          bedrooms|         bathrooms|       sqft_living|          sqft_lot|            floors|          waterfront|               view|         condition|        sqft_above|     sqft_basement|         yr_built|     yr_renovated|           street|        city|statezip|country|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------+--------+-------+\n",
      "|  count|             4600|              4600|              4600|              4600|              4600|              4600|                4600|               4600|              4600|              4600|              4600|             4600|             4600|             4600|        4600|    4600|   4600|\n",
      "|   mean|551962.9884732141|3.4008695652173913|2.1608152173913044|2139.3469565217392|14852.516086956522|1.5120652173913043|0.007173913043478261|0.24065217391304347|3.4517391304347824|1827.2654347826087|312.08152173913044|1970.786304347826|808.6082608695652|             null|        null|    null|   null|\n",
      "| stddev|563834.7025471414|0.9088481155258177|0.7837810746502791| 963.2069157608655| 35884.43614480964|0.5382883772969884| 0.08440377189474309| 0.7784047172125206| 0.677229767559275| 862.1689769625966| 464.1372280666068|29.73184839009962|979.4145364007456|             null|        null|    null|   null|\n",
      "|    min|              0.0|               0.0|               0.0|               370|               638|               1.0|                   0|                  0|                 1|               370|                 0|             1900|                0|     1 View Ln NE|      Algona|WA 98001|    USA|\n",
      "|    max|          2.659E7|               9.0|               8.0|             13540|           1074218|               3.5|                   1|                  4|                 5|              9410|              4820|             2014|             2014|Valley View Trail|Yarrow Point|WA 98354|    USA|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary',\n",
       " 'price',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'sqft_living',\n",
       " 'sqft_lot',\n",
       " 'floors',\n",
       " 'waterfront',\n",
       " 'view',\n",
       " 'condition',\n",
       " 'sqft_above',\n",
       " 'sqft_basement',\n",
       " 'yr_built',\n",
       " 'yr_renovated',\n",
       " 'street',\n",
       " 'city',\n",
       " 'statezip',\n",
       " 'country']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+-----------+---------+-------------+----------+----------+----------+------------+\n",
      "|summary|    price| bedrooms| bathrooms|sqft_living| sqft_lot|sqft_basement|      view|sqft_above| condition|yr_renovated|\n",
      "+-------+---------+---------+----------+-----------+---------+-------------+----------+----------+----------+------------+\n",
      "|  count|   4600.0|   4600.0|    4600.0|     4600.0|   4600.0|       4600.0|    4600.0|    4600.0|    4600.0|        4600|\n",
      "|   mean| 551963.0|3.4008696| 2.1608152|   2139.347|14852.516|     312.0815|0.24065217| 1827.2654|  3.451739|         808|\n",
      "| stddev| 563834.7|0.9088481|0.78378105|   963.2069|35884.438|    464.13724| 0.7784047|   862.169|0.67722976|         979|\n",
      "|    min|      0.0|      0.0|       0.0|      370.0|    638.0|          0.0|       0.0|     370.0|       1.0|           0|\n",
      "|    25%| 322500.0|      3.0|      1.75|     1460.0|   5000.0|          0.0|       0.0|    1190.0|       3.0|           0|\n",
      "|    50%|460886.94|      3.0|      2.25|     1980.0|   7683.0|          0.0|       0.0|    1590.0|       3.0|           0|\n",
      "|    75%| 654950.0|      4.0|       2.5|     2620.0|  11000.0|        610.0|       0.0|    2300.0|       4.0|        1999|\n",
      "|    max|  2.659E7|      9.0|       8.0|    13540.0|1074218.0|       4820.0|       4.0|    9410.0|       5.0|        2014|\n",
      "+-------+---------+---------+----------+-----------+---------+-------------+----------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number, format_string\n",
    "summary_stats.select(summary_stats['summary'], summary_stats['price'].cast('float'), \n",
    "                     summary_stats['bedrooms'].cast('float'), summary_stats['bathrooms'].cast('float'),\n",
    "                    summary_stats['sqft_living'].cast('float'), summary_stats['sqft_lot'].cast('float'),\n",
    "                    summary_stats['sqft_basement'].cast('float'), summary_stats['view'].cast('float'),\n",
    "                    summary_stats['sqft_above'].cast('float'), summary_stats['condition'].cast('float'),\n",
    "                    summary_stats['yr_renovated'].cast('integer')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn('bigger_house', when(df['sqft_lot'] > 14852.516, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|view|count|\n",
      "+----+-----+\n",
      "|   1|   69|\n",
      "|   3|  116|\n",
      "|   4|   70|\n",
      "|   2|  205|\n",
      "|   0| 4140|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('view').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|bedrooms|count|\n",
      "+--------+-----+\n",
      "|     8.0|    2|\n",
      "|     0.0|    2|\n",
      "|     7.0|   14|\n",
      "|     1.0|   38|\n",
      "|     4.0| 1531|\n",
      "|     3.0| 2032|\n",
      "|     2.0|  566|\n",
      "|     6.0|   61|\n",
      "|     5.0|  353|\n",
      "|     9.0|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('bedrooms').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|bigger_house|count|\n",
      "+------------+-----+\n",
      "|           1|  752|\n",
      "|           0| 3848|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('bigger_house').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|bigger_house|       avg(price)|\n",
      "+------------+-----------------+\n",
      "|           1|686039.8739938992|\n",
      "|           0|525760.8528413132|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('bigger_house', 'bedrooms', 'bathrooms', 'view', 'price').groupBy(['bigger_house'])\\\n",
    "        .agg({'price': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|bathrooms|   avg_price|\n",
      "+---------+------------+\n",
      "|     0.75|  293,955.94|\n",
      "|      1.0|  332,620.63|\n",
      "|      1.5|  411,044.28|\n",
      "|     1.75|  459,549.65|\n",
      "|     1.25|  461,150.00|\n",
      "|      2.0|  526,857.97|\n",
      "|     2.25|  537,178.43|\n",
      "|     5.75|  540,000.00|\n",
      "|      2.5|  572,705.21|\n",
      "|     2.75|  643,424.57|\n",
      "|      3.0|  705,563.58|\n",
      "|     3.75|  901,380.30|\n",
      "|      3.5|  903,002.61|\n",
      "|     3.25|  944,241.29|\n",
      "|      5.0|  946,171.83|\n",
      "|      4.0|  959,036.09|\n",
      "|      0.0|1,195,324.00|\n",
      "|      4.5|1,379,316.63|\n",
      "|     4.25|1,396,876.35|\n",
      "|     6.25|1,444,000.00|\n",
      "+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = df.groupBy('bathrooms').mean('price').orderBy('avg(price)', desc=True).withColumnRenamed('avg(price)', 'avg_price')\n",
    "\n",
    "a.select('bathrooms', format_number('avg_price', 2).alias('avg_price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'street', 'city', 'statezip', 'country', 'bigger_house']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    0     0     0     0     0     0     0     0     0  ...     0     0  \\\n",
       "0   1.0  1.0  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  ...  0.02  0.02   \n",
       "1   NaN  0.2  0.20  0.20  0.20  0.20  0.20  0.20  0.20  0.20  ...  0.14  0.14   \n",
       "2   NaN  NaN  0.33  0.33  0.33  0.33  0.33  0.33  0.33  0.33  ...  0.46  0.46   \n",
       "3   NaN  NaN   NaN  0.43  0.43  0.43  0.43  0.43  0.43  0.43  ...   NaN  0.29   \n",
       "4   NaN  NaN   NaN   NaN  0.05  0.05  0.05  0.05  0.05  0.05  ...   NaN   NaN   \n",
       "5   NaN  NaN   NaN   NaN   NaN  0.15  0.15  0.15  0.15  0.15  ...   NaN   NaN   \n",
       "6   NaN  NaN   NaN   NaN   NaN   NaN  0.14  0.14  0.14  0.14  ...   NaN   NaN   \n",
       "7   NaN  NaN   NaN   NaN   NaN   NaN   NaN  0.23  0.23  0.23  ...   NaN   NaN   \n",
       "8   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  0.03  0.03  ...   NaN   NaN   \n",
       "9   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  0.37  ...   NaN   NaN   \n",
       "10  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN   \n",
       "11  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   NaN   NaN   \n",
       "\n",
       "       0     0     0     0     0     0     0     0  \n",
       "0   0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  \n",
       "1   0.14  0.14  0.14  0.14  0.14  0.14  0.14  0.14  \n",
       "2   0.46  0.46  0.46  0.46  0.46  0.46  0.46  0.46  \n",
       "3   0.29  0.29  0.29  0.29  0.29  0.29  0.29  0.29  \n",
       "4   0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  \n",
       "5    NaN  0.47  0.47  0.47  0.47  0.47  0.47  0.47  \n",
       "6    NaN   NaN -0.02 -0.02 -0.02 -0.02 -0.02 -0.02  \n",
       "7    NaN   NaN   NaN -0.06 -0.06 -0.06 -0.06 -0.06  \n",
       "8    NaN   NaN   NaN   NaN -0.40 -0.40 -0.40 -0.40  \n",
       "9    NaN   NaN   NaN   NaN   NaN  0.41  0.41  0.41  \n",
       "10   NaN   NaN   NaN   NaN   NaN   NaN -0.16 -0.16  \n",
       "11   NaN   NaN   NaN   NaN   NaN   NaN   NaN  1.00  \n",
       "\n",
       "[12 rows x 144 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the correlation\n",
    "from pyspark.ml.stat import Correlation\n",
    "import pandas as pd\n",
    "\n",
    "x = df.columns[1:13]\n",
    "corr_df = pd.DataFrame()\n",
    "\n",
    "for i in x:\n",
    "    corr = []\n",
    "    for j in x:\n",
    "        corr.append(round(df.stat.corr(i, j), 2))\n",
    "        corr_df = pd.concat([corr_df, pd.Series(corr)], axis=1)\n",
    "        \n",
    "# corr_df.columns = x\n",
    "# corr_df.insert(0, '', x)\n",
    "# corr_df.set_index('')\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3271099182877348"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('price', 'bathrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20033628937567646"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('price', 'bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03491453732641387"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('price', 'condition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = df.columns[2: 13]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price and bedrooms corr score:: 0.2\n",
      "price and bathrooms corr score:: 0.33\n",
      "price and sqft_living corr score:: 0.43\n",
      "price and sqft_lot corr score:: 0.05\n",
      "price and floors corr score:: 0.15\n",
      "price and waterfront corr score:: 0.14\n",
      "price and view corr score:: 0.23\n",
      "price and condition corr score:: 0.03\n",
      "price and sqft_above corr score:: 0.37\n",
      "price and sqft_basement corr score:: 0.21\n",
      "price and yr_built corr score:: 0.02\n"
     ]
    }
   ],
   "source": [
    "for i in feature_cols:\n",
    "    print(f'price and {i} corr score:: {round(df.corr(\"price\", i),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature Enginerring\n",
    "from pyspark.sql.functions import pow\n",
    "df = df.withColumn('all_rooms', df['bedrooms']+df['bathrooms'])\n",
    "df = df.withColumn('bedrooms^2', pow(df['bedrooms'], 2))\n",
    "df = df.withColumn('bathrooms^2', pow(df['bathrooms'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('sqft_living^2', pow(df['sqft_living'], 2))\n",
    "df = df.withColumn('sqft_above^2', pow(df['sqft_above'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machice Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML library here with PySpark is not yet on the same level as what scikit-learn provides but it will serve the purpose for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'price',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'sqft_living',\n",
       " 'sqft_lot',\n",
       " 'floors',\n",
       " 'waterfront',\n",
       " 'view',\n",
       " 'condition',\n",
       " 'sqft_above',\n",
       " 'sqft_basement',\n",
       " 'yr_built',\n",
       " 'yr_renovated',\n",
       " 'street',\n",
       " 'city',\n",
       " 'statezip',\n",
       " 'country',\n",
       " 'bigger_house',\n",
       " 'all_rooms',\n",
       " 'bedrooms^2',\n",
       " 'bathrooms^2',\n",
       " 'sqft_living^2',\n",
       " 'sqft_above^2']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_2_drop = ['price', 'street', 'city', 'date', 'statezip', 'country', 'condition', 'bedrooms', 'bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(prediction):\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(prediction)\n",
    "    print(\"RMSE on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, Normalizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# specify the stages\n",
    "vec_assembler = VectorAssembler(inputCols=[i for i in df.columns if i not in col_2_drop], outputCol='features')\n",
    "normalizer = Normalizer(inputCol='features', outputCol='features_norm')\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price')\n",
    "\n",
    "# build the actual pipeline\n",
    "pipe = Pipeline(stages=[vec_assembler, normalizer, lr])\n",
    "\n",
    "# fit the data using the pipeline\n",
    "model = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 501676\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(train)\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 482527\n"
     ]
    }
   ],
   "source": [
    "regression_metrics(model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+------------------+\n",
      "|        price|            features|       features_norm|        prediction|\n",
      "+-------------+--------------------+--------------------+------------------+\n",
      "|237227.857143|[2200.0,9397.0,2....|[3.21411809905429...| 535086.4179687053|\n",
      "|     242500.0|[1200.0,9720.0,1....|[5.89248460208566...| 330298.6412654491|\n",
      "|     260000.0|[1480.0,8625.0,1....|[4.77772794115911...|360717.66457646433|\n",
      "|     275000.0|[1180.0,10277.0,1...|[5.99234043111752...| 270814.0957511896|\n",
      "|     284000.0|[1800.0,23103.0,1...|[3.92831973159204...|452885.00970491115|\n",
      "|     285000.0|[2090.0,10834.0,1...|[4.40596531231741...|484920.15456246585|\n",
      "|     287200.0|[1850.0,19966.0,1...|[5.10638872021590...|419386.25289527234|\n",
      "|     295000.0|[1630.0,1368.0,2....|[5.22192343453728...| 374119.9623330943|\n",
      "|     300000.0|[2540.0,5050.0,2....|[2.78388425584094...|  614520.316567895|\n",
      "|     313000.0|[1340.0,7912.0,1....|[5.27688598584067...| 394333.4855443053|\n",
      "|     315000.0|[1160.0,9180.0,1....|[6.09566830535882...| 271685.8725390015|\n",
      "|     335000.0|[1350.0,2560.0,1....|[5.23782244880050...| 369219.7009253949|\n",
      "|     335000.0|[1580.0,16215.0,1...|[4.47530985507483...|408733.16012785584|\n",
      "|     365000.0|[1090.0,6435.0,1....|[6.48715884206483...|292325.47799338866|\n",
      "|     367500.0|[3110.0,7231.0,2....|[2.27365478550047...| 757898.8738006037|\n",
      "|     382500.0|[1560.0,8700.0,1....|[4.53271962287927...| 382569.3096858673|\n",
      "|     385000.0|[1320.0,1327.0,2....|[6.43647862456852...| 337643.8768443782|\n",
      "|     400000.0|[3630.0,42884.0,1...|[2.55648598472667...| 848393.8013184476|\n",
      "|     407500.0|[1930.0,10460.0,2...|[3.66375772811196...|493891.13527826313|\n",
      "|     419000.0|[1570.0,6700.0,1....|[4.50385490660516...|437700.96221968625|\n",
      "+-------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(['price', 'features', 'features_norm', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Performance is Generally Poor, I'll try scikit learn algorithms\n",
    "\n",
    "Specifically GradientBoostingRegressor PS: pyspark also provide this but i just want to experiment. You never know what you'll find right??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=400, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Much Exploration, Scikit Learn Algorithms don't accept DenseVector which pyspark returns hence back to pyspark :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:58251)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ABUTON\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ABUTON\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:58251)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-87dabd911ec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGBTRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBTRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'features_norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# build the actual pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec_assembler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgbr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\ml\\regression.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, featuresCol, labelCol, predictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, subsamplingRate, checkpointInterval, lossType, maxIter, stepSize, seed, impurity, featureSubsetStrategy)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \"\"\"\n\u001b[0;32m   1109\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBTRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.regression.GBTRegressor\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         self._setDefault(maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,\n\u001b[0;32m   1112\u001b[0m                          \u001b[0mmaxMemoryInMB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcacheNodeIds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsamplingRate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[0;32m   1650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    981\u001b[0m          \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \"\"\"\n\u001b[1;32m--> 983\u001b[1;33m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[0;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[1;32m--> 937\u001b[1;33m         \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[1;34m\"server ({0}:{1})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:58251)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbr = GBTRegressor(featuresCol='features_norm', labelCol='price')\n",
    "\n",
    "# build the actual pipeline\n",
    "pipe = Pipeline(stages=[vec_assembler, normalizer, gbr])\n",
    "\n",
    "# fit the data using the pipeline\n",
    "model = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 230273\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(train)\n",
    "regression_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much Better Now still can improve though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
